How to profile an OpenMP application
------------------------------------

Callgraph profiling
...................

On RTL simulator and virtual platform, a PC-based callgraph for a single core can be generated with the following command: ::

  make clean all run KCG=0

This will generate the callgraph for the specified core number, 0 in this case.

The callgraph can be seen with KCachegrind tool with (the actual path to the kcg file is displayed at the end of the simulation and may be different): ::

  kcachegrind build/kcg.txt

Kcachegrind is an open-source tool whose documentation can be found here: http://kcachegrind.sourceforge.net/html/Home.html

Here are a few hints:

- On the top-right corner, you'll find the event currently being watched. The platform is registering the events from the core HW counters and thus any HW event can be viewed. The first one (*PC*) is just based on the PC information and is equivalent to *Instructions*. All the others are HW events, you can refer to the core datasheet for more information. The most common event is *Cycles* as it will show the cycles spent.

And the most interesting panels:

- **Flat Profile**. It is by default the left panel. This view will show the time spent in each function, both including everything or just including the time really spent in the function not the one spent in the called functions

- **Source Code**. This shows the time spent on each source code line, which is nice for identifying the hot spots. This will also show the functions called, and how many time.

- **Call graph**. This shows how the time spent in a function is spent between the functions called by this one.

Execution traces profiling
..........................

With OMP on the virtual platform, more information about the execution context can be obtained with: ::

  make clean all run pulpRtVersion=profile0

This activates a version of the runtime instrumented for generating context information through HW traces.

They can be visualized either with Android traceview tool (only on Ubuntu) with this command: ::

  traceview build/tvDump

Or as a textual callgraph: ::

  cat build/cg_cluster0_core0.txt

The actual paths to the files might also be different and are displayed at the end of the simulation.

Kernel statistics
.................

The main issue of the execution traces is that it does not give any application context, and thus it is hard to link the results with the application code.

To overcome this, it is possible to instrumentate the application to give the missing context, by classifying the source into kernels. 

To use this support, the following file must be included: ::

  #include "hwTrace.h"

The file is located here in the SDK: *install/include/hwTrace.h*. More information about this support can be found in this file.

An example can be found in apps/gomp_tests/kernelTrace.

Here are the main things to know:

- In order to better identify the performances of an application, this support allows breaking the whole code into kernels and sees the performances of each kernel.

- Kernels must first be declared with the following call: ::

    pulp_trace_kernel_declare(0, "kernel 0");

  The first argument is the kernel identifier which must be unique and which will be used later on to associate traces to this kernel. The second argument is a string that will be used by the profiling tools to report results for this kernel.

- The execution of a kernel can then be caught with the following sequence: ::
  
    pulp_trace_kernel_start(0, 1);
    // Put the kernel code here
    pulp_trace_kernel_stop(0, 1);

  All instructions executed between these 2 calls will be accounted on this kernel. The first argument to both calls is the kernel identifier specified when declaring the kernel. The second argument should be 0 if we just want the report about the instructions or 1 if we also want the report for all core HW counters (which is more intrusive). If everything goes well, you should see something like this at the end of the simulation: ::

    +----+----------+----+------------+------------+------------+--------------+--------------+----------+-----------+-------+---------+-------------+------+------+------+--------+-----------+--------+--------+------------+------------+-----------+
    | ID |     name | nb | avg cycles | min cycles | max cycles | total cycles | Cycles | Instructions | LD_Stall | Jmp_Stall | IMISS | WBRANCH | WBRANCH_CYC |   LD |   ST | JUMP | BRANCH | DELAY_NOP | LD_EXT | ST_EXT | LD_EXT_CYC | ST_EXT_CYC | TCDM_CONT |
    +----+----------+----+------------+------------+------------+--------------+--------------+----------+-----------+-------+---------+-------------+------+------+------+--------+-----------+--------+--------+------------+------------+-----------+
    |  0 | kernel 0 | 10 |       1532 |       1365 |       2680 |        15329 |        13233 |      179 |         0 |  1666 |       0 |           0 | 1896 | 3310 |    0 |      0 |         0 |     30 |   1240 |          0 |          0 |        27 |
    +----+----------+----+------------+------------+------------+--------------+--------------+----------+-----------+-------+---------+-------------+------+------+------+--------+-----------+--------+--------+------------+------------+-----------+

- The kernel instructions can also be seen using traceview by opening the traceview report (see above). The kernel execution will now be displayed as a specific event which allows better understanding the global execution.

- If the kernel is executed several times, and thus the execution goes several times through the sequence start -> stop, each execution will be registered and the number of executions will be reported (column *nb*). In this case, the column *avg cycles* is an average over all these executions, *min cycles* and *max cycles* are the mininal and maximal cycles count seen over the executions. All the other statistics are cumulated over the executions.

- In case some instructions executed inside a start -> stop sequence should not be accounted on the kernel (for example to not include some debug code), it is possible to pause and resume the profiling with these calls: ::

    // Kernel code
    pulp_trace_kernel_pause(0, 1);
    // This code here won't be accounted on the kernel
    pulp_trace_kernel_resume(0, 1);
    // Kernel code

The arguments are the same as for start and stop

- The number under *Cycles* is the number of cycles where the core is active, while *total cycles* is also including the number of cycles where the core is sleeping.

  
