Reference
=========

Configuration
-------------

When compiling an application relying on the makefiles provided by the SDK, several options can be specified either in the application makefile or on the command line to specify the Pulp configuration.

The configuration includes selecting the platform, the architecture, as well as the runtime. All the available options can be displayed from the application folder with this command: ::

  $ make config

For example, the pulp architecture and the stack size can be specified with this command: ::

  $ make config pulpArchi=pulp4 stackSize=2048

You should see in the new printed table that your options has been taken into account. A few options like the pulp architecture have impacts on default values, so you may see different values for options that you didn't specify.

You can also set these options directly from the application makefile to make them permanent, which can be useful for example for the stack size: ::

  PULP_APP = test
  PULP_APP_SRCS = test.c

  stackSize=2048

  include $(PULP_SDK_HOME)/install/rules/pulp.mk

Tools flags
-----------

The flags used for compiling and running the application are automatically generated by the SDK makefiles depending on the specified options.

To see the generated flags, you can execute this command: ::

  $ make flags

If you add an option to the command or to the makefile, you should see the impact on the flags.

An application can specify additional flags, for example to specify the level of optimization. You can for example add this in the application makefile: ::

  PULP_APP = test
  PULP_APP_SRCS = test.c
  PULP_CFLAGS = -O3

Note that the flags you specify there are not displayed by the *make config* command.

Here is the list of available flags that you can add to the application makefile:

================ ======================
Flag             Description
================ ======================
PULP_DEFS        Specifies definitions (=D<name>=<value>)
PULP_INC_PATHS   Specifies include folders where to look for headers
PULP_CFLAGS      Specifies compiler flags
PULP_LDFLAGS     Specifies linker flags
PLT_OPT          Specifies platform runner flags
================ ======================

Makefile targets
----------------

The common makefiles provides several targets for compiling, running and profiling applications. You can get the full list of targets and their description by executing this command: ::

  $ make help

You can also execute it with the configuration options specified, as some targets may be specific to a configuration.


Gvsoc traces
------------

The virtual platform allows dumping architecture events to help the developper debugging his application by better showing what is happening in the system.

For example, it can show instructions being executed, DMA transfers, events generated, memory accesses and so on.

This feature can be enabled and configured through the *--gv-trace* option. This option takes an argument which specifies the path in the architecture where the traces must be enabled. All components included in this path will dump traces. Several paths can be specified by using several times the option. Here is an example that activates instruction traces for core 0 and core 1: ::

  $ make run PLT_OPT="--gv-trace=pe0/iss --gv-trace=pe1/iss"

The traces are dumped into a file which is named *trace.txt* and which is located inside the build folder. To find out where is the build folder you can have a look at the command which is used to launch the simulator.

The trace file should look like the following: ::

  27600000: [/soc/fabric/cluster0/pe1/iss  ] r9 =00000000         1c0000a4 l.movhi r9,0
  27600000: [/soc/fabric/cluster0/pe0/iss  ] r9 =00000000         1c0000a4 l.movhi r9,0

There is usually one line per event, although an event can sometimes takes several lines to display more information.

The number on the left gives the timestamp of the event, in picoseconds. This is not using cycles because different blocks like clusters can have different frequencies.

The second part, which is a string, gives the path in the architecture where the event occured. This is useful to differentiate blocks of the same kind that generate the same event. This path can also be used with the *--gv-trace* option to reduce the number of events.

The third part, which is also a string, is the information dumped by the event, and is totally specific to this event. In our example, the core simulator is just printing information about the instruction that has been executed.

One difficulty is usually to find out which paths should be activated to get the needed information. One method is to dump all the events with *--gv-trace=.**, then find out which one are insteresting and then put them on the command line. Here are the paths for the main components:

================================== ===============================
Path                               Description
================================== ===============================
/soc/fabric/cluster0/pe0           Processing element, useful to see the IOs made by the core, and the instruction it executes. You can add */iss* to just get instruction events
/soc/fabric/cluster0/ckg           Hardware synchronizer events, useful for debugging inter-core synchronization mechanisms
/soc/fabric/cluster0/pcache        Shared program cache accesses
/soc/fabric/cluster0/l1ico         Shared L1 interconnect
/soc/fabric/cluster0/l1_X          L1 memory banks (the X should be replaced by the bank number)
/soc/fabric/l2                     L2 memory accesses
/soc/fabric/cluster0/dma           DMA events
================================== ===============================


Runtime
-------

The runtime is made of a first layer that contains architecture files (mainly register mapping), HAL files (mainly header files hiding register accesses) and a system library providing higher-level features like memory allocation.

You can have access to these 3 layers by including the following file: ::

  include <rt/pulp.h>


Getting cycles
--------------

The best way to get a cycle count is to use the cluster timer. Once activated, this 64-bits counter is counting cycles whatever the state of the cores. Just be careful about the fact this is a shared counter, so only one core should reset it while all the cores can read it.

To use it, you must first activate it, which can be done with the following code: ::

  if (get_core_id() == 0) {
    reset_timer();
    start_timer();
  }
  
This is usually core 0 which activates it at the beginning of the application.

Then you can execute the following code to get the number of cycles (on 32bits): ::

  if (get_core_id() == 0) {
    stop_timer();
    time = get_time();
  }

The first line is used to stop the timer. It can be used together with *start_timer* to stop and resume the timer in order to cumulate the time over several portions of code. However, you may not use that if you want to share the timer between several cores.


Performance counters
--------------------

In order to get more understanding about the application, you can use the HW counters that give you detailed information about several events like cache misses, stalls and so on.

Here is an example showing how to use them: ::
  
  #include "omp.h"
  #include <rt/pulp.h>

  int main()
  {
    hw_perf_t perf;
    hw_perf_init(&perf);

    while (hw_perf_step(&perf))
    {
      hw_perf_start(&perf);
      
      #pragma omp parallel
      {
        int i;
        for (i=0; i<1000; i++) 
          {
            *(volatile int *)0x10000000;
          }
        for (i=0; i<1000; i++) 
          {
            *(volatile int *)0x1c000000;
          }
      }
    
      hw_perf_stop(&perf);
      hw_perf_commit(&perf);
    }

    int i;
    printf("HW performance counters results\n");

    for (i=0; i<hw_perf_nb_events(&perf); i++)
    {
      printf("%20s: %d\n", hw_perf_get_name(&perf, i), hw_perf_get_value(&perf, i));
    }

    return 0;
  }

Each core contains a set of HW counters, whose numbers depends on the platform. The prototyping platforms, such as the virtual platform, the RTL simulator and the FPGA contains one counter per event, so that you can get all the information in one shot. The chips contains a single counter that can be configured to count a single event, so you will have to iterate several times to get all the information.

The API provided in the *hwPerf.h* file allows you to write a portable code that will get all the information, whatever the platform. For that you have to first initialize a structure with *hw_perf_init* and loop on the code you want to benchmark until *hw_perf_step* returns 0. Inside the code to be profiled, you have to use *hw_perf_start* and *hw_perf_stop* to define the exact region to be profiled and *hw_perf_commit* to read the events from the counters and put them in the structure. Once done, there are other functions to help you getting and displaying the results.

Once this test is executed, you should get something like: ::

              CYCLES: 12966
               INSTR: 6325
            LD_STALL: 13
           JMP_STALL: 0
               IMISS: 638
             WBRANCH: 0
         WBRANCH_CYC: 0
                  LD: 2044
                  ST: 66
                JUMP: 8
              BRANCH: 24
           DELAY_NOP: 10
              LD_EXT: 1001
              ST_EXT: 4
          LD_EXT_CYC: 5001
          ST_EXT_CYC: 0
           TCDM_CONT: 998

CYCLES are the number of cycles while the core is not in sleep mode, so it should correspond to the timer count except if the core was in idle mode.

INSTR gives the number of instructions. If you compare it to the number of cycles, you can quickly know if you reach a good ratio of instruction per cycle, which tells you how much the core was stalled (ideally you have number of instructions equal to the number of cycles).

The rest of the events gives you more details about the reasons why the core was stalled, you can get more details in the core architecture documentation.

Note that the performance counters have been added recently and are only available since the pulp4 architecture. They are for example not available in the mia architecture which is the default one.
